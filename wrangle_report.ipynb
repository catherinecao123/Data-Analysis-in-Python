{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Wrangle Process Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Gathering, collect datasets from three sources\n",
    "- dowload the twitter-archive-enhanced.csv dataset from Udacity project\n",
    "- set up Tweeter developer account, and obtain the credentials <br>\n",
    "  query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON in a text file.<br>\n",
    "- use requests and zipfile library and get function to retrieve the image-predictions.zip, image-predictions.tsv file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Accessing\n",
    "\n",
    "- access and exam the datasets visually.<br>\n",
    "`df.head()`, `df.tail()` can show the first 5 records and last 5 records of the data, <br>\n",
    "`df.sample(n=int)`can randomly sample any number of records randomly. I did not use `df` to show the whole dataset, for big dataset, it can be overwhelming \n",
    "\n",
    "- explore the data structure and attributes programmatically.<br>\n",
    "`df.info()` shows the size of the dataset, the data type of the columns, the none null counts of the columns,<br> \n",
    "`df.describe(include=all)` can show the general statistics of the data, counts, unique, top, frequency, mean,  first, second, third quantile etc. statistics for both numeric and character variables.<br>\n",
    "`df.denominator.value_counts()`,`df.numerator.value_counts()` help describe the categories and counts for each category level, so I can tell the incorrect denominator and numerator easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning \n",
    "\n",
    "- make copies of the original dataset to prepare for data wrangling \n",
    "\n",
    "##### **Define the quality issues in the datasets**\n",
    "Quality issues with content. Low quality data is also known as dirty data.<br>\n",
    "**a)twitter-archive-enhanced.csv** \n",
    "- 1.convert tweet_id from int to string, use `.astype(str)`\n",
    "- 2.trim the timestamp column to only date, convert the data type from object to datetime, rename the column name to date\n",
    "- 3.extract the method or device from the source column use `str.extract()`function with regular expression pattern\n",
    "- 4.normalize the numerator by divided by the denominator, replace extreme and unreasonable value with mean\n",
    "- 5.change the denominator from various numbers to 10, base of 10 is fixed in this rating system\n",
    "- 6.some dog's name is not correct, like 'None', 'a','the' but seems we cannot recover all the names from the text column\n",
    "- 7.remove extra dog stage description. there are 14 tweet_id have 2 descriptions of dog age stage\n",
    "- 8.drop records before 2018-8-1\n",
    "\n",
    "**b)tweet_json.txt file**\n",
    "- 1.drop tweet_id, keep id_str, these two are duplicated with different data types and names, only keep one\n",
    "- 2.drop columns with more than 30% null values, as well as unnecessary columns \n",
    "- 3.use the full text column to find the name of the dog\n",
    "\n",
    "**c)image-prediction.tsv**\n",
    "- 1.convert tweet_id from integer to string\n",
    "\n",
    "##### **Define the tidiness issues in the three datasets**\n",
    "Tidiness issues with structure that prevent easy analysis. Untidy data is also known as messy data. Tidy data requirements that each variable forms a column, each observation forms a row. each type of observational unit forms a table.\n",
    "- 1.melt doggo, floofer, pupper, puppo columns in twitter-archive-enhanced.csv to one column with a variable name dog_stage \n",
    "- 2.merge the df2_clean table with df1_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Storing and Acting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned data df1_clean use `df1_clean.to_csv('twitter_archive_master.csv', index=False)`\n",
    "\n",
    "Store the df3_image use `df3_image.to_csv('prediction_image.csv, index=False)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
